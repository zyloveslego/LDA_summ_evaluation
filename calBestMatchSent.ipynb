{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import traceback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import nltk.data\n",
    "from itertools import combinations\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"/Users/zhouyou/Downloads/bbc news dataset\"\n",
    "all_path = \"/Users/zhouyou/Downloads/summ/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "file_dirs = os.listdir(corpus_path)\n",
    "# news dataset dir\n",
    "corpus_text = []\n",
    "for file_dir in file_dirs:\n",
    "    if os.path.isdir(corpus_path + '/' + file_dir):\n",
    "        # different category dir\n",
    "        files = os.listdir(corpus_path + '/' + file_dir)\n",
    "        count = 0\n",
    "        for file in files:\n",
    "            try:\n",
    "                # 每种取了200篇\n",
    "                if count < 200:\n",
    "                    # txt file\n",
    "                    f = open(corpus_path + '/' + file_dir + \"/\" + file)\n",
    "                    text_content = \"\"\n",
    "                    next(f)\n",
    "                    for line in f.readlines():\n",
    "                        if line != \"\\n\":\n",
    "                            text_content = text_content + line\n",
    "                    text_content = text_content.replace(\"\\n\", \"\")\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    break\n",
    "                corpus_text.append(text_content)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "p_stemmer = WordNetLemmatizer()\n",
    "\n",
    "tokennized_text = []\n",
    "tokennized_word = []\n",
    "\n",
    "for i in corpus_text:\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    stopped_tokens = [i.lower() for i in tokens if not i in en_stop]\n",
    "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    stemmed_tokens = [p_stemmer.lemmatize(i) for i in stopped_tokens]\n",
    "    final_tokens = []\n",
    "    for i in stemmed_tokens:\n",
    "        if len(i) > 1:\n",
    "            final_tokens.append(i)\n",
    "    # print(stemmed_tokens)\n",
    "    tokennized_word.extend(final_tokens)\n",
    "    tokennized_text.append(final_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算词频，删除频率太低的词\n",
    "corpus_word_frequency = {}\n",
    "for i in tokennized_word:\n",
    "    if i in corpus_word_frequency:\n",
    "        corpus_word_frequency[i] = corpus_word_frequency[i] + 1\n",
    "    else:\n",
    "        corpus_word_frequency.setdefault(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_frequency_word = []\n",
    "for i in corpus_word_frequency.keys():\n",
    "    if corpus_word_frequency[i] <= 2:\n",
    "        low_frequency_word.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokennized_text:\n",
    "    for j in low_frequency_word:\n",
    "        if j in i:\n",
    "            i.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_text = []\n",
    "for i in tokennized_text:\n",
    "    temp = \"\"\n",
    "    for word in i:\n",
    "        temp = temp + \" \" + word\n",
    "    rebuild_text.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读文本\n",
    "def readfile(path):\n",
    "    # 遍历文件夹\n",
    "    files = os.listdir(path)\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        raw_text = ''\n",
    "        raw_text_list = []\n",
    "        if not os.path.isdir(file):\n",
    "            # print(\"File name: \" + file)\n",
    "            f = open(path + \"/\" + file)\n",
    "            for line in f.readlines():\n",
    "                searchObj = re.findall(r'SNO=(.*?)>(.*?)</S>', line)\n",
    "                if searchObj:\n",
    "                    # print(searchObj)\n",
    "                    # print(searchObj[0][0])\n",
    "                    if searchObj[0][0] == \"\\\"1\\\"\":\n",
    "                        raw_text = raw_text + searchObj[0][1] + '.' + ' '\n",
    "                        raw_text_list.append(searchObj[0][1])\n",
    "                    else:\n",
    "                        raw_text = raw_text + searchObj[0][1] + ' '\n",
    "                        raw_text_list.append(searchObj[0][1])\n",
    "                    # print(s)\n",
    "            yield file, raw_text, raw_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoc(document_path):\n",
    "    per_raw_text = readfile(document_path)\n",
    "\n",
    "    all_raw_text = []\n",
    "    all_filenames = []\n",
    "    all_raw_text_list = []\n",
    "\n",
    "    for filename, raw_text, raw_text_list in per_raw_text:\n",
    "        all_filenames.append(filename)\n",
    "        all_raw_text.append(raw_text)\n",
    "        all_raw_text_list.append(raw_text_list)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    p_stemmer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    doc = []\n",
    "    sentences = sent_tokenizer.tokenize(all_raw_text[0])\n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.lemmatize(i) for i in stopped_tokens]\n",
    "        doc.append(stemmed_tokens)\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = getDoc(all_path + \"2/docsent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cal_cos_sim(temp, document_topic_list, document_topic_porb):\n",
    "    cos_sim = []\n",
    "    count = 0\n",
    "    judge_topic_list = []\n",
    "    \n",
    "    for j in temp:\n",
    "        a, b = j\n",
    "        judge_topic_list.append(a)\n",
    "    union_set = set(document_topic_list).union(set(judge_topic_list))\n",
    "    \n",
    "    document_topic_porb_kv = {}\n",
    "    for j in document_topic_porb:\n",
    "        a, b = j\n",
    "        document_topic_porb_kv.setdefault(a, b)\n",
    "        \n",
    "    judge_topic_list_kv = {}\n",
    "    for j in temp:\n",
    "        a, b = j\n",
    "        judge_topic_list_kv.setdefault(a, b)\n",
    "    \n",
    "    document_topic_porb_cal_coss = []\n",
    "    judge_topic_list_cal_coss = []\n",
    "    for k in union_set:\n",
    "        document_topic_porb_cal_coss.append(document_topic_porb_kv.get(k, 0))\n",
    "        judge_topic_list_cal_coss.append(judge_topic_list_kv.get(k, 0))\n",
    "        \n",
    "        \n",
    "    cos_sim.append(cosine_similarity(np.array([document_topic_porb_cal_coss]), np.array([judge_topic_list_cal_coss]))[0][0])\n",
    "    return cos_sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(tokennized_text)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in tokennized_text]\n",
    "    \n",
    "# generate LDA model\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=20)\n",
    "\n",
    "# document_topic: 文本的分布\n",
    "doc_input = []\n",
    "for i in doc:\n",
    "    doc_input = doc_input + i\n",
    "doc_bow = dictionary.doc2bow(doc_input)\n",
    "document_topic = ldamodel.get_document_topics(doc_bow)\n",
    "        \n",
    "# 计算相似度\n",
    "document_topic_porb = document_topic\n",
    "document_topic_list = []\n",
    "for i in document_topic:\n",
    "    a, b = i\n",
    "    document_topic_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=2\n",
      "best sentence=[3, 17]\n",
      "i=3\n",
      "best sentence=[3, 14, 17]\n",
      "i=4\n",
      "best sentence=[3, 9, 12, 17]\n",
      "i=5\n",
      "best sentence=[2, 3, 9, 11, 17]\n",
      "i=6\n",
      "best sentence=[2, 3, 9, 14, 17, 18]\n",
      "i=7\n",
      "best sentence=[2, 3, 9, 11, 12, 13, 17]\n",
      "i=8\n",
      "best sentence=[2, 3, 6, 7, 9, 14, 17, 18]\n",
      "i=9\n",
      "best sentence=[2, 3, 6, 7, 9, 11, 12, 13, 17]\n",
      "i=10\n",
      "best sentence=[2, 3, 6, 7, 8, 9, 12, 13, 17, 18]\n",
      "i=11\n",
      "best sentence=[3, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18]\n",
      "i=12\n",
      "best sentence=[4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 17, 18]\n",
      "i=13\n",
      "best sentence=[1, 2, 3, 4, 6, 7, 9, 11, 12, 15, 16, 17, 18]\n",
      "i=14\n",
      "best sentence=[1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 14, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 15):\n",
    "    combs = list(combinations(range(0, 18), i))\n",
    "    # 所有拍立组合\n",
    "    bestCosSim = 0\n",
    "    bestComb = None\n",
    "    for comb in combs:\n",
    "        # 某一个排列组合计算cos_sim\n",
    "        randomCombSent = []\n",
    "        for sentNum in comb:\n",
    "            randomCombSent = randomCombSent + doc[sentNum]\n",
    "        # 计算cos_sim后比较，大就存下来\n",
    "        summ_bow = dictionary.doc2bow(randomCombSent)      \n",
    "        res = ldamodel.get_document_topics(bow=summ_bow)\n",
    "        cos_sim = cal_cos_sim(res, document_topic_list, document_topic_porb)\n",
    "        if cos_sim[0] > bestCosSim:\n",
    "            bestCosSim = cos_sim\n",
    "            bestComb = comb\n",
    "        \n",
    "    print(\"i=\" + str(i))\n",
    "    select_sent = []\n",
    "    for sent in bestComb:\n",
    "        select_sent.append(sent + 1)\n",
    "    print(\"best sentence=\" + str(select_sent))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
