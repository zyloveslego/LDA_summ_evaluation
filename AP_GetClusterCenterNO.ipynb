{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/zhouyou/Documents/PHD/summarization/resources/w2v_model/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = \"/Users/zhouyou/Downloads/doc/docsent\"\n",
    "corpus_add = \"/Users/zhouyou/Downloads/doc/corpus_add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path):\n",
    "    # 遍历文件夹\n",
    "    files = os.listdir(path)\n",
    "    files.sort()\n",
    "    for file in files:\n",
    "        raw_text = ''\n",
    "        raw_text_list = []\n",
    "        if not os.path.isdir(file):\n",
    "            # print(\"File name: \" + file)\n",
    "            f = open(path + \"/\" + file)\n",
    "            for line in f.readlines():\n",
    "                searchObj = re.findall(r'SNO=(.*?)>(.*?)</S>', line)\n",
    "                if searchObj:\n",
    "                    # print(searchObj)\n",
    "                    # print(searchObj[0][0])\n",
    "                    if searchObj[0][0] == \"\\\"1\\\"\":\n",
    "                        raw_text = raw_text + searchObj[0][1] + '.' + ' '\n",
    "                        raw_text_list.append(searchObj[0][1])\n",
    "                    else:\n",
    "                        raw_text = raw_text + searchObj[0][1] + ' '\n",
    "                        raw_text_list.append(searchObj[0][1])\n",
    "                    # print(s)\n",
    "            yield file, raw_text, raw_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读原文的文件.\n",
    "per_raw_text = readfile(document_path)\n",
    "\n",
    "all_raw_text = []\n",
    "all_filenames = []\n",
    "all_raw_text_list = []\n",
    "\n",
    "for filename, raw_text, raw_text_list in per_raw_text:\n",
    "    all_filenames.append(filename)\n",
    "    all_raw_text.append(raw_text)\n",
    "    all_raw_text_list.append(raw_text_list)\n",
    "\n",
    "# print(all_raw_text[0])\n",
    "# print(len(all_raw_text))\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# p_stemmer = PorterStemmer()\n",
    "# p_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# wordnet 提取词干效果更好\n",
    "p_stemmer = WordNetLemmatizer()\n",
    "\n",
    "# texts 才是最后要放入dic的list\n",
    "\n",
    "# 目标文本加入corpus\n",
    "texts = []\n",
    "for i in all_raw_text:\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    stemmed_tokens = [p_stemmer.lemmatize(i) for i in stopped_tokens]\n",
    "    # print(stemmed_tokens)\n",
    "    texts.append(stemmed_tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chief Executive to renew ties with our trading partners . The Chief Executive , Mr Tung Chee Hwa , will visit Malaysia , Singapore , the United States , Japan , Belgium and the UK in the next two months to renew ties with our trading partners and update them on developments in Hong Kong after the handover . During these visits , Mr Tung will call on political and business leaders , and brief them on the smooth transition in Hong Kong to demonstrate our pride to return to China and our confidence in  \"Hong Kong people ruling Hong Kong \" under the  \"one country , two systems \" concept . Mr Tung will be in Kuala Lumpur on September 3 and call on Prime Minister Dr Mahathir Mohamad . He will be in Singapore on September 4 and 5 and call on Prime Minister Goh Chok Tong . Mr Tung will be in the United States from September 9 to 11 . He will visit Washington and New York . In Washington , Mr Tung looks forward to meeting with President Clinton . Mr Tung will also officiate at the opening ceremony of the new office premises of the Hong Kong Economic and Trade Office in Washington . Mr Tung will be in Tokyo from October 15 to 17 . He will call on ministers , parliamentarians and senior officials of the Japanese Government . He will then leave for Europe on October 19 and visit Brussels and London before returning to Hong Kong on October 23 . In Brussels , Mr Tung will call on the President of the European Commission , Mr Jacques Santer , and attend the Trade Development Council Annual Dinner in Europe . He also hopes to meet with Belgian ministers . In London , Mr Tung looks forward to meeting with Prime Minister Tony Blair . Mr Tung will be speaking at a number of functions in various destinations . He will reassure overseas audiences of our continued role as an active regional and global player in international trade , commerce and finance . He will also welcome international investors to come to Hong Kong to take advantage of the enormous business opportunities here as Hong Kong becomes part of China . \n"
     ]
    }
   ],
   "source": [
    "print(all_raw_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 随机文本加入corpus\n",
    "# corpus_add_files = os.listdir(corpus_add)\n",
    "# firstFile = False\n",
    "\n",
    "# for file in corpus_add_files:\n",
    "#     try:\n",
    "#         corpus_add_file = open(corpus_add + \"/\" + file)\n",
    "#         # print(corpus_add_file)\n",
    "#         for line in corpus_add_file.readlines():\n",
    "#             # print(line)\n",
    "#             tokens = tokenizer.tokenize(line)\n",
    "#             stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#             stemmed_tokens = [p_stemmer.lemmatize(i) for i in stopped_tokens]\n",
    "\n",
    "#         texts.append(stemmed_tokens)\n",
    "#         # print(len(stemmed_tokens))\n",
    "#         # break\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "csvFile = open(\"/Users/zhouyou/Downloads/all-the-news/articles1.csv\", \"r\")\n",
    "reader = csv.reader(csvFile)\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_corpus = []\n",
    "for item in reader:\n",
    "    if reader.line_num == 1:\n",
    "        continue\n",
    "    else:\n",
    "        if reader.line_num > 2000:\n",
    "            break\n",
    "        # print(item[9])\n",
    "        line = item[9]\n",
    "        raw_text_corpus.append(line)\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.lemmatize(i) for i in stopped_tokens]\n",
    "\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "raw_text_corpus.append(all_raw_text[0])\n",
    "print(len(raw_text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "word_frequence = vectorizer.fit_transform(raw_text_corpus)\n",
    "words = vectorizer.get_feature_names()\n",
    "tfidf = transformer.fit_transform(word_frequence)\n",
    "# print(word_frequence[99])\n",
    "# print(word_frequence[99].shape)\n",
    "# tfidf 就是每篇文章的表示\n",
    "print(tfidf.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# AP算法计算聚类中心数\n",
    "ap = AffinityPropagation(preference=-50).fit(tfidf.toarray())\n",
    "cluster_centers_indices = ap.cluster_centers_indices_\n",
    "labels = ap.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttf = {}\n",
    "for sent in texts:\n",
    "    for i in sent:\n",
    "#         print(i)\n",
    "        if i in texttf:\n",
    "            texttf[i] = texttf[i] + 1\n",
    "        else:\n",
    "            texttf[i] = 1\n",
    "            \n",
    "# print(len(texttf))\n",
    "\n",
    "AP_input = []            \n",
    "for i in texttf.keys():\n",
    "    try:\n",
    "        wordVec = model[i].tolist()\n",
    "        wordVec.append(texttf[i])\n",
    "        AP_input.append(wordVec)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(AP_input))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "# AP算法计算聚类中心数\n",
    "ap = AffinityPropagation(preference=-50).fit(AP_input)\n",
    "cluster_centers_indices = ap.cluster_centers_indices_\n",
    "labels = ap.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
